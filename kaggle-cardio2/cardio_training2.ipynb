{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cardio training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stemanz/ml-datasets/blob/master/kaggle-cardio2/cardio_training2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jDoWavJKBuMC",
        "colab_type": "code",
        "outputId": "fa5efca6-24ae-43f3-a9bf-ff1b4536fa84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Stemanz/ml-datasets.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-datasets'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 40 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QNhL0g5eDmnl",
        "colab_type": "code",
        "outputId": "5b6f2b38-b863-4cc2-d8b8-95eb778ec4c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ml-datasets  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6PxLupoyEGF3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "class DatasetHandler():\n",
        "    \"\"\"Iterator that gives back the dataset in slices useful for\n",
        "    training, testing and validation.\n",
        "    \n",
        "    Data should already have been pre-processed.\n",
        "    \n",
        "    input\n",
        "    =====\n",
        "    \n",
        "    dataset | the .npz file where the dataset is stored. <dataset> is\n",
        "            assumed to contain the arrays \"inputs\" and \"targets\"\n",
        "              \n",
        "    batch_size | defines the batch size for training. If unspecified,\n",
        "            the dataset is loaded in a whole batch\n",
        "    \n",
        "    returns\n",
        "    =======\n",
        "    \n",
        "    Sliced (in batches) <inputs> and <targets>\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset, batch_size=None, classes_num=2):\n",
        "        \n",
        "        npz = np.load(dataset)\n",
        "        self.inputs  = npz[\"inputs\"].astype(np.float32)\n",
        "        self.targets = npz[\"targets\"].astype(np.int8)\n",
        "        self.classes_num = classes_num\n",
        "        \n",
        "        # Counts the batch number. If None, we are either validating\n",
        "        # or testing (we are not training), so we take it all\n",
        "        if batch_size is None:\n",
        "            self.batch_size = self.inputs.shape[0] # n. of rows\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "        \n",
        "        self.curr_batch = 0\n",
        "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
        "        \n",
        "    def __next__(self):\n",
        "        \n",
        "        if self.curr_batch >= self.batch_count:\n",
        "            self.curr_batch = 0\n",
        "            raise StopIteration\n",
        "        \n",
        "        start = self.curr_batch * self.batch_size\n",
        "        stop  = (self.curr_batch + 1) * self.batch_size\n",
        "        batch_slice   = slice(start, stop)\n",
        "        inputs_batch  = self.inputs[batch_slice]\n",
        "        targets_batch = self.targets[batch_slice]\n",
        "        self.curr_batch += 1\n",
        "        \n",
        "        # one-hot encoding\n",
        "        targets_one_hot = np.zeros((targets_batch.shape[0], self.classes_num))\n",
        "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
        "        \n",
        "        return inputs_batch, targets_one_hot\n",
        "        \n",
        "    def __iter__(self):\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2Tp47UwCCJX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**"
      ]
    },
    {
      "metadata": {
        "id": "ICGR2pckCA2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"ml-datasets/kaggle-cardio2/heart.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VczzBVw4CKPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "f38fff6e-5e10-46cc-dbb4-487a71f46c39"
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            "age         303 non-null int64\n",
            "sex         303 non-null int64\n",
            "cp          303 non-null int64\n",
            "trestbps    303 non-null int64\n",
            "chol        303 non-null int64\n",
            "fbs         303 non-null int64\n",
            "restecg     303 non-null int64\n",
            "thalach     303 non-null int64\n",
            "exang       303 non-null int64\n",
            "oldpeak     303 non-null float64\n",
            "slope       303 non-null int64\n",
            "ca          303 non-null int64\n",
            "thal        303 non-null int64\n",
            "target      303 non-null int64\n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 33.2 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hx10TiaTCRgD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46c96677-5a08-478a-dd71-8d407d92967b"
      },
      "cell_type": "code",
      "source": [
        "# checking if dataset is balanced\n",
        "ratio = sum(df[\"target\"]) / len(df[\"target\"])\n",
        "print(f\"{round(ratio, 2)}%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.54%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CQNvlbH1DT2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target_col = \"target\"\n",
        "targets = np.array(df[target_col].copy())\n",
        "del df[target_col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nGk_cD0bDd44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "unscaled_inputs = np.array(df) # avoids later Warnings\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(unscaled_inputs)\n",
        "\n",
        "scaled_inputs = scaler.transform(unscaled_inputs)\n",
        "# putting back the thing into a DataFrame and * CREATING A CHECKPOINT *\n",
        "df_processed_unscaled = df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMcT_TvhDrIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06affc80-b12a-4814-c30f-ae26dc6079ce"
      },
      "cell_type": "code",
      "source": [
        "unscaled_inputs.shape[0] # that's not too much -.-'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "WfOFc8gMDvlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2f828ef2-efdc-4a63-edf7-363d9ded4404"
      },
      "cell_type": "code",
      "source": [
        "# shuffling and dividing the dataset\n",
        "# np.array version\n",
        "\n",
        "# parameters\n",
        "# ==========\n",
        "train_size = .8\n",
        "validation_size = .1\n",
        "dataset = scaled_inputs # requires: np.array\n",
        "\n",
        "\n",
        "# determining sizes\n",
        "index_range  = dataset.shape[0]\n",
        "train_n      = int(index_range * train_size)\n",
        "validation_n = int(index_range * validation_size)\n",
        "test_n       = index_range - train_n - validation_n\n",
        "\n",
        "# pulling random indexes\n",
        "indices = list(range(index_range))\n",
        "train_indices      = [random.choice(indices) for _ in range(train_n)]\n",
        "validation_indices = [random.choice(indices) for _ in range(validation_n)]\n",
        "test_indices       = [random.choice(indices) for _ in range(test_n)]\n",
        "assert len(train_indices) + len(validation_indices) + len(test_indices) == index_range\n",
        "\n",
        "# slicing dataset and targets\n",
        "train_df = dataset[train_indices]\n",
        "train_targets = targets[train_indices]\n",
        "validation_df = dataset[validation_indices]\n",
        "validation_targets = targets[validation_indices]\n",
        "test_df = dataset[test_indices]\n",
        "test_targets = targets[test_indices]\n",
        "\n",
        "# are outputs binary? If so, printing some stats\n",
        "if len(set(targets)) < 3:\n",
        "    print(f\"Ones in TOTAL: {round((sum(targets) / len(targets))*100, 2)}%\")\n",
        "    print(f\"Ones in train: {round((sum(train_targets) / len(train_targets))*100, 2)}%\")\n",
        "    print(f\"Ones in validation: {round((sum(validation_targets) / len(validation_targets))*100, 2)}%\")\n",
        "    print(f\"Ones in test: {round((sum(test_targets) / len(test_targets))*100, 2)}%\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ones in TOTAL: 54.46%\n",
            "Ones in train: 54.55%\n",
            "Ones in validation: 50.0%\n",
            "Ones in test: 58.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "43Rg42pGD6Ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepping and saving the datasets (automatically adds .npz)\n",
        "#np.savez(\"train\", inputs=np.array(train_df), targets=train_targets)\n",
        "#np.savez(\"validation\", inputs=np.array(validation_df), targets=validation_targets)\n",
        "#np.savez(\"test\", inputs=np.array(test_df), targets=test_targets)\n",
        "\n",
        "# saved datasets features:\n",
        "#Ones in TOTAL: 54.46%\n",
        "#Ones in train: 54.55%\n",
        "#Ones in validation: 50.0%\n",
        "#Ones in test: 58.06%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TfopUfO0Eeum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a78cc81-246d-4df5-bef7-2f767375fd8c"
      },
      "cell_type": "code",
      "source": [
        "print(f\"Number of inputs for deep neural network: {np.array(train_df).shape[1]}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of inputs for deep neural network: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0KzNgsE1E9cR",
        "colab_type": "code",
        "outputId": "64e7b3e6-7bf5-4bb2-c7fc-40d36d8cb113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3137
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    sess.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Reset the default graph, so you can fiddle with the hyperparameters\n",
        "# and then rerun the code.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# parameters\n",
        "# ==========\n",
        "train_npz = \"train.npz\"\n",
        "validation_npz = \"validation.npz\"\n",
        "test_npz = \"test.npz\"\n",
        "\n",
        "# Input size depends on the number of input variables.\n",
        "input_size = 13\n",
        "# Output size is 2, as we one-hot encoded the targets.\n",
        "output_size = 2\n",
        "# Choose a hidden_layer_size\n",
        "hidden_layer_size = 128\n",
        "# Guess what?\n",
        "learning_rate=0.0001\n",
        "# Choose the batch size\n",
        "batch_size = 5\n",
        "\n",
        "# Set early stopping mechanisms\n",
        "max_epochs = 300\n",
        "prev_validation_loss = 9999999.\n",
        "\n",
        "# ==========\n",
        "\n",
        "# Create the placeholders\n",
        "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
        "targets = tf.placeholder(tf.int32, [None, output_size])\n",
        "\n",
        "# Stacking the layers of the model\n",
        "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
        "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
        "outputs_1 = tf.nn.tanh(tf.matmul(inputs, weights_1) + biases_1)\n",
        "\n",
        "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
        "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
        "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
        "\n",
        "weights_final = tf.get_variable(\"weights_final\", [hidden_layer_size, output_size])\n",
        "biases_final = tf.get_variable(\"biases_final\", [output_size])\n",
        "HIDDEN_LAYERS = 2\n",
        "# We will incorporate the softmax activation into the loss\n",
        "outputs = tf.matmul(outputs_2, weights_final) + biases_final # ← change here\n",
        "\n",
        "# Use the softmax cross entropy loss with logits\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
        "mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "# Get a 0 or 1 for every input indicating whether it output the correct answer\n",
        "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
        "\n",
        "# Optimize with Adam\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
        "\n",
        "# Create a session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# Initialize the variables\n",
        "initializer = tf.global_variables_initializer()\n",
        "sess.run(initializer)\n",
        "\n",
        "\n",
        "# let's call our class\n",
        "train_data = DatasetHandler(train_npz, batch_size)\n",
        "validation_data = DatasetHandler(validation_npz)\n",
        "\n",
        "print(\"Running the deep neural network model.\")\n",
        "print(\"======================================\")\n",
        "print(f\"Batch size: {batch_size}\\n\\\n",
        "Hidden layers: {HIDDEN_LAYERS}\\n\\\n",
        "Neurons per layer: {hidden_layer_size}\\n\")\n",
        "\n",
        "t0 = time.time()\n",
        "# Create the loop for epochs \n",
        "for epoch_counter in range(max_epochs):\n",
        "    \n",
        "    print(f\"Current epoch: {epoch_counter}\", end=\"\\r\")    \n",
        "    # initializing variables for current epoch\n",
        "    curr_epoch_loss     = 0.\n",
        "    \n",
        "    # learning with train dataset\n",
        "    for input_batch, target_batch in train_data:\n",
        "        _, batch_loss = sess.run(\n",
        "            [optimize, mean_loss], \n",
        "            feed_dict={inputs: input_batch, targets: target_batch}\n",
        "        )\n",
        "        curr_epoch_loss += batch_loss\n",
        "    \n",
        "    curr_epoch_loss /= train_data.batch_count #average for batch\n",
        "    \n",
        "    # forward propagating only the validation dataset\n",
        "    for input_batch, target_batch in validation_data:\n",
        "        validation_loss, validation_accuracy = sess.run(\n",
        "            [mean_loss, accuracy],\n",
        "            feed_dict={inputs: input_batch, targets: target_batch}\n",
        "        )\n",
        "    \n",
        "    print(f\"Epoch: {epoch_counter}\", end=\" \")\n",
        "    print(f\"Training loss: {round(curr_epoch_loss, 2)}\", end=\" \")\n",
        "    print(f\"Validation loss: {round(float(validation_loss), 2)}\", end=\" \")\n",
        "    print(f\"Validation accuracy: {round(validation_accuracy * 100, 2)}%\", end=\"\\n\")\n",
        "    \n",
        "    # Trigger early stopping if validation loss begins increasing.\n",
        "    if validation_loss > prev_validation_loss:\n",
        "        break\n",
        "        \n",
        "    # Store this epoch's validation loss to be used as previous in the next iteration.\n",
        "    prev_validation_loss = validation_loss\n",
        "    \n",
        "t1 = time.time()\n",
        "print(f\"End of training. Training took {round(t1 - t0, 2)} seconds.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running the deep neural network model.\n",
            "======================================\n",
            "Batch size: 5\n",
            "Hidden layers: 2\n",
            "Neurons per layer: 128\n",
            "\n",
            "Epoch: 0 Training loss: 0.92 Validation loss: 0.73 Validation accuracy: 50.0%\n",
            "Epoch: 1 Training loss: 0.67 Validation loss: 0.51 Validation accuracy: 66.67%\n",
            "Epoch: 2 Training loss: 0.51 Validation loss: 0.38 Validation accuracy: 86.67%\n",
            "Epoch: 3 Training loss: 0.42 Validation loss: 0.31 Validation accuracy: 90.0%\n",
            "Epoch: 4 Training loss: 0.36 Validation loss: 0.26 Validation accuracy: 86.67%\n",
            "Epoch: 5 Training loss: 0.33 Validation loss: 0.23 Validation accuracy: 93.33%\n",
            "Epoch: 6 Training loss: 0.31 Validation loss: 0.21 Validation accuracy: 93.33%\n",
            "Epoch: 7 Training loss: 0.29 Validation loss: 0.2 Validation accuracy: 93.33%\n",
            "Epoch: 8 Training loss: 0.28 Validation loss: 0.19 Validation accuracy: 93.33%\n",
            "Epoch: 9 Training loss: 0.27 Validation loss: 0.18 Validation accuracy: 93.33%\n",
            "Epoch: 10 Training loss: 0.26 Validation loss: 0.17 Validation accuracy: 93.33%\n",
            "Epoch: 11 Training loss: 0.26 Validation loss: 0.17 Validation accuracy: 93.33%\n",
            "Epoch: 12 Training loss: 0.25 Validation loss: 0.16 Validation accuracy: 93.33%\n",
            "Epoch: 13 Training loss: 0.24 Validation loss: 0.16 Validation accuracy: 93.33%\n",
            "Epoch: 14 Training loss: 0.24 Validation loss: 0.15 Validation accuracy: 93.33%\n",
            "Epoch: 15 Training loss: 0.23 Validation loss: 0.15 Validation accuracy: 93.33%\n",
            "Epoch: 16 Training loss: 0.23 Validation loss: 0.15 Validation accuracy: 93.33%\n",
            "Epoch: 17 Training loss: 0.22 Validation loss: 0.14 Validation accuracy: 93.33%\n",
            "Epoch: 18 Training loss: 0.22 Validation loss: 0.14 Validation accuracy: 93.33%\n",
            "Epoch: 19 Training loss: 0.21 Validation loss: 0.14 Validation accuracy: 93.33%\n",
            "Epoch: 20 Training loss: 0.21 Validation loss: 0.13 Validation accuracy: 93.33%\n",
            "Epoch: 21 Training loss: 0.21 Validation loss: 0.13 Validation accuracy: 93.33%\n",
            "Epoch: 22 Training loss: 0.2 Validation loss: 0.13 Validation accuracy: 93.33%\n",
            "Epoch: 23 Training loss: 0.2 Validation loss: 0.13 Validation accuracy: 93.33%\n",
            "Epoch: 24 Training loss: 0.19 Validation loss: 0.13 Validation accuracy: 96.67%\n",
            "Epoch: 25 Training loss: 0.19 Validation loss: 0.12 Validation accuracy: 96.67%\n",
            "Epoch: 26 Training loss: 0.19 Validation loss: 0.12 Validation accuracy: 96.67%\n",
            "Epoch: 27 Training loss: 0.19 Validation loss: 0.12 Validation accuracy: 96.67%\n",
            "Epoch: 28 Training loss: 0.18 Validation loss: 0.12 Validation accuracy: 96.67%\n",
            "Epoch: 29 Training loss: 0.18 Validation loss: 0.12 Validation accuracy: 96.67%\n",
            "Epoch: 30 Training loss: 0.18 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 31 Training loss: 0.17 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 32 Training loss: 0.17 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 33 Training loss: 0.17 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 34 Training loss: 0.17 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 35 Training loss: 0.16 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 36 Training loss: 0.16 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 37 Training loss: 0.16 Validation loss: 0.11 Validation accuracy: 96.67%\n",
            "Epoch: 38 Training loss: 0.16 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 39 Training loss: 0.15 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 40 Training loss: 0.15 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 41 Training loss: 0.15 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 42 Training loss: 0.15 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 43 Training loss: 0.14 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 44 Training loss: 0.14 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 45 Training loss: 0.14 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 46 Training loss: 0.14 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 47 Training loss: 0.14 Validation loss: 0.1 Validation accuracy: 96.67%\n",
            "Epoch: 48 Training loss: 0.13 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 49 Training loss: 0.13 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 50 Training loss: 0.13 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 51 Training loss: 0.13 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 52 Training loss: 0.13 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 53 Training loss: 0.12 Validation loss: 0.09 Validation accuracy: 96.67%\n",
            "Epoch: 54 Training loss: 0.12 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 55 Training loss: 0.12 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 56 Training loss: 0.12 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 57 Training loss: 0.12 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 58 Training loss: 0.11 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 59 Training loss: 0.11 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 60 Training loss: 0.11 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 61 Training loss: 0.11 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 62 Training loss: 0.11 Validation loss: 0.09 Validation accuracy: 100.0%\n",
            "Epoch: 63 Training loss: 0.11 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 64 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 65 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 66 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 67 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 68 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 69 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 70 Training loss: 0.1 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 71 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 72 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 73 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 74 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 75 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 76 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 77 Training loss: 0.09 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 78 Training loss: 0.08 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 79 Training loss: 0.08 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 80 Training loss: 0.08 Validation loss: 0.08 Validation accuracy: 100.0%\n",
            "Epoch: 81 Training loss: 0.08 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 82 Training loss: 0.08 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 83 Training loss: 0.08 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 84 Training loss: 0.08 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 85 Training loss: 0.08 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 86 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 87 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 88 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 89 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 90 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 91 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 92 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 93 Training loss: 0.07 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 94 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 95 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 96 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 97 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 98 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 99 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 100 Training loss: 0.06 Validation loss: 0.07 Validation accuracy: 100.0%\n",
            "Epoch: 101 Training loss: 0.06 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 102 Training loss: 0.06 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 103 Training loss: 0.06 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 104 Training loss: 0.06 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 105 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 106 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 107 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 108 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 109 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 110 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 111 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 112 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 113 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 114 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 115 Training loss: 0.05 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 116 Training loss: 0.04 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 117 Training loss: 0.04 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 118 Training loss: 0.04 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 119 Training loss: 0.04 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 120 Training loss: 0.04 Validation loss: 0.06 Validation accuracy: 100.0%\n",
            "Epoch: 121 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 122 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 123 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 124 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 125 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 126 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 127 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 128 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 129 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 130 Training loss: 0.04 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 131 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 132 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 133 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 134 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 135 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 136 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 137 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 138 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 139 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 140 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 141 Training loss: 0.03 Validation loss: 0.05 Validation accuracy: 100.0%\n",
            "Epoch: 142 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 143 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 144 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 145 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 146 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 147 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 148 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 149 Training loss: 0.03 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 150 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 151 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 152 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 153 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 154 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 155 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 156 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 157 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 158 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 159 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 160 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 161 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 162 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 163 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 164 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 165 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 166 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 167 Training loss: 0.02 Validation loss: 0.04 Validation accuracy: 100.0%\n",
            "Epoch: 168 Training loss: 0.02 Validation loss: 0.03 Validation accuracy: 100.0%\n",
            "Epoch: 169 Training loss: 0.02 Validation loss: 0.03 Validation accuracy: 100.0%\n",
            "Epoch: 170 Training loss: 0.02 Validation loss: 0.03 Validation accuracy: 100.0%\n",
            "Epoch: 171 Training loss: 0.02 Validation loss: 0.03 Validation accuracy: 100.0%\n",
            "Epoch: 172 Training loss: 0.02 Validation loss: 0.03 Validation accuracy: 100.0%\n",
            "End of training. Training took 20.26 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcLm_IfqpDWa",
        "colab_type": "code",
        "outputId": "caee9c4e-99d3-41b7-9a03-8603dbefcceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# forward propagating the test dataset\n",
        "test_data = DatasetHandler(test_npz)\n",
        "\n",
        "for input_batch, target_batch in test_data:\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={inputs: input_batch, targets: target_batch}\n",
        "    )\n",
        "\n",
        "test_accuracy_percent = round(test_accuracy * 100, 2)\n",
        "print(f\"Test accuracy: {test_accuracy_percent}%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 96.77%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D_HEdqxwiD3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trying now other classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVjWqkvTnl2Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reg = LogisticRegression(solver=\"liblinear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-dTJIXkUntAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "npz = np.load(\"train.npz\")\n",
        "x_train = npz[\"inputs\"].astype(np.float32)\n",
        "y_train = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_uCdz5RobG0",
        "colab_type": "code",
        "outputId": "8049b727-4983-409f-cbe6-6f8b6ff6af7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "reg.fit(x_train, y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "afXHI6IXoeyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# validation dataset\n",
        "npz = np.load(\"validation.npz\")\n",
        "x_valid = npz[\"inputs\"].astype(np.float32)\n",
        "y_valid = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWp7x55xom2Y",
        "colab_type": "code",
        "outputId": "b965f962-e8a9-465d-d593-7704b2f1f027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "reg.score(x_valid, y_valid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "ixggUmIeorya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test dataset\n",
        "npz = np.load(\"test.npz\")\n",
        "x_test = npz[\"inputs\"].astype(np.float32)\n",
        "y_test = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyVhHk9io6hg",
        "colab_type": "code",
        "outputId": "f3b73b62-56e1-4357-e63e-ec6539ebd65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "reg.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7262857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "dK7xw_8io9EJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}