{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cardio training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stemanz/ml-datasets/blob/master/cardio_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jDoWavJKBuMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "89e56ebe-f82a-4e3e-9552-8a7f3e1c9edb"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Stemanz/ml-datasets.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-datasets'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)   \u001b[K\rremote: Counting objects:  33% (2/6)   \u001b[K\rremote: Counting objects:  50% (3/6)   \u001b[K\rremote: Counting objects:  66% (4/6)   \u001b[K\rremote: Counting objects:  83% (5/6)   \u001b[K\rremote: Counting objects: 100% (6/6)   \u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)   \u001b[K\rremote: Compressing objects:  33% (2/6)   \u001b[K\rremote: Compressing objects:  50% (3/6)   \u001b[K\rremote: Compressing objects:  66% (4/6)   \u001b[K\rremote: Compressing objects:  83% (5/6)   \u001b[K\rremote: Compressing objects: 100% (6/6)   \u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "Unpacking objects:  16% (1/6)   \rUnpacking objects:  33% (2/6)   \rUnpacking objects:  50% (3/6)   \rUnpacking objects:  66% (4/6)   \rUnpacking objects:  83% (5/6)   \rremote: Total 6 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6)   \rUnpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QNhL0g5eDmnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8de0b53e-33c0-4fd4-b48a-6711488fe511"
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ml-datasets  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6PxLupoyEGF3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "class DatasetHandler():\n",
        "    \"\"\"Iterator that gives back the dataset in slices useful for\n",
        "    training, testing and validation.\n",
        "    \n",
        "    Data should already have been pre-processed.\n",
        "    \n",
        "    input\n",
        "    =====\n",
        "    \n",
        "    dataset | the .npz file where the dataset is stored. <dataset> is\n",
        "            assumed to contain the arrays \"inputs\" and \"targets\"\n",
        "              \n",
        "    batch_size | defines the batch size for training. If unspecified,\n",
        "            the dataset is loaded in a whole batch\n",
        "    \n",
        "    returns\n",
        "    =======\n",
        "    \n",
        "    Sliced (in batches) <inputs> and <targets>\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset, batch_size=None, classes_num=2):\n",
        "        \n",
        "        npz = np.load(dataset)\n",
        "        self.inputs  = npz[\"inputs\"].astype(np.float32)\n",
        "        self.targets = npz[\"targets\"].astype(np.int8)\n",
        "        self.classes_num = classes_num\n",
        "        \n",
        "        # Counts the batch number. If None, we are either validating\n",
        "        # or testing (we are not training), so we take it all\n",
        "        if batch_size is None:\n",
        "            self.batch_size = self.inputs.shape[0] # n. of rows\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "        \n",
        "        self.curr_batch = 0\n",
        "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
        "        \n",
        "    def __next__(self):\n",
        "        \n",
        "        if self.curr_batch >= self.batch_count:\n",
        "            self.curr_batch = 0\n",
        "            raise StopIteration\n",
        "        \n",
        "        start = self.curr_batch * self.batch_size\n",
        "        stop  = (self.curr_batch + 1) * self.batch_size\n",
        "        batch_slice   = slice(start, stop)\n",
        "        inputs_batch  = self.inputs[batch_slice]\n",
        "        targets_batch = self.targets[batch_slice]\n",
        "        self.curr_batch += 1\n",
        "        \n",
        "        # one-hot encoding\n",
        "        targets_one_hot = np.zeros((targets_batch.shape[0], self.classes_num))\n",
        "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
        "        \n",
        "        return inputs_batch, targets_one_hot\n",
        "        \n",
        "    def __iter__(self):\n",
        "        return self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KzNgsE1E9cR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3155
        },
        "outputId": "0e138d3d-3742-4821-bcda-f0c2b587d132"
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    sess.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Reset the default graph, so you can fiddle with the hyperparameters\n",
        "# and then rerun the code.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# parameters\n",
        "# ==========\n",
        "train_npz = \"ml-datasets/train.npz\"\n",
        "validation_npz = \"ml-datasets/validation.npz\"\n",
        "test_npz = \"ml-datasets/test.npz\"\n",
        "\n",
        "# Input size depends on the number of input variables.\n",
        "input_size = 11\n",
        "# Output size is 2, as we one-hot encoded the targets.\n",
        "output_size = 2\n",
        "# Choose a hidden_layer_size\n",
        "hidden_layer_size = 128\n",
        "# Guess what?\n",
        "learning_rate=0.0001\n",
        "# Choose the batch size\n",
        "batch_size = 100\n",
        "\n",
        "# Set early stopping mechanisms\n",
        "max_epochs = 300\n",
        "prev_validation_loss = 9999999.\n",
        "\n",
        "# ==========\n",
        "\n",
        "# Create the placeholders\n",
        "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
        "targets = tf.placeholder(tf.int32, [None, output_size])\n",
        "\n",
        "# Stacking the layers of the model\n",
        "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
        "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
        "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
        "\n",
        "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
        "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
        "outputs_2 = tf.nn.tanh(tf.matmul(outputs_1, weights_2) + biases_2)\n",
        "\n",
        "weights_final = tf.get_variable(\"weights_final\", [hidden_layer_size, output_size])\n",
        "biases_final = tf.get_variable(\"biases_final\", [output_size])\n",
        "HIDDEN_LAYERS = 2\n",
        "# We will incorporate the softmax activation into the loss\n",
        "outputs = tf.matmul(outputs_2, weights_final) + biases_final # â† change here\n",
        "\n",
        "# Use the softmax cross entropy loss with logits\n",
        "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
        "mean_loss = tf.reduce_mean(loss)\n",
        "\n",
        "# Get a 0 or 1 for every input indicating whether it output the correct answer\n",
        "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
        "\n",
        "# Optimize with Adam\n",
        "optimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
        "\n",
        "# Create a session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# Initialize the variables\n",
        "initializer = tf.global_variables_initializer()\n",
        "sess.run(initializer)\n",
        "\n",
        "\n",
        "# let's call our class\n",
        "train_data = DatasetHandler(train_npz, batch_size)\n",
        "validation_data = DatasetHandler(validation_npz)\n",
        "\n",
        "print(\"Running the deep neural network model.\")\n",
        "print(\"======================================\")\n",
        "print(f\"Batch size: {batch_size}\\n\\\n",
        "Hidden layers: {HIDDEN_LAYERS}\\n\\\n",
        "Neurons per layer: {hidden_layer_size}\\n\")\n",
        "\n",
        "t0 = time.time()\n",
        "# Create the loop for epochs \n",
        "for epoch_counter in range(max_epochs):\n",
        "    \n",
        "    print(f\"Current epoch: {epoch_counter}\", end=\"\\r\")    \n",
        "    # initializing variables for current epoch\n",
        "    curr_epoch_loss     = 0.\n",
        "    \n",
        "    # learning with train dataset\n",
        "    for input_batch, target_batch in train_data:\n",
        "        _, batch_loss = sess.run(\n",
        "            [optimize, mean_loss], \n",
        "            feed_dict={inputs: input_batch, targets: target_batch}\n",
        "        )\n",
        "        curr_epoch_loss += batch_loss\n",
        "    \n",
        "    curr_epoch_loss /= train_data.batch_count #average for batch\n",
        "    \n",
        "    # forward propagating only the validation dataset\n",
        "    for input_batch, target_batch in validation_data:\n",
        "        validation_loss, validation_accuracy = sess.run(\n",
        "            [mean_loss, accuracy],\n",
        "            feed_dict={inputs: input_batch, targets: target_batch}\n",
        "        )\n",
        "    \n",
        "    print(f\"Epoch: {epoch_counter}\", end=\" \")\n",
        "    print(f\"Training loss: {round(curr_epoch_loss, 2)}\", end=\" \")\n",
        "    print(f\"Validation loss: {round(float(validation_loss), 2)}\", end=\" \")\n",
        "    print(f\"Validation accuracy: {round(validation_accuracy * 100, 2)}%\", end=\"\\n\")\n",
        "    \n",
        "    # Trigger early stopping if validation loss begins increasing.\n",
        "    if validation_loss > prev_validation_loss:\n",
        "        break\n",
        "        \n",
        "    # Store this epoch's validation loss to be used as previous in the next iteration.\n",
        "    prev_validation_loss = validation_loss\n",
        "    \n",
        "t1 = time.time()\n",
        "print(f\"End of training. Training took {round(t1 - t0, 2)} seconds.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running the deep neural network model.\n",
            "======================================\n",
            "Batch size: 100\n",
            "Hidden layers: 2\n",
            "Neurons per layer: 128\n",
            "\n",
            "Epoch: 0 Training loss: 0.63 Validation loss: 0.61 Validation accuracy: 66.69%\n",
            "Epoch: 1 Training loss: 0.6 Validation loss: 0.59 Validation accuracy: 68.96%\n",
            "Epoch: 2 Training loss: 0.59 Validation loss: 0.57 Validation accuracy: 71.63%\n",
            "Epoch: 3 Training loss: 0.57 Validation loss: 0.56 Validation accuracy: 72.66%\n",
            "Epoch: 4 Training loss: 0.57 Validation loss: 0.56 Validation accuracy: 72.81%\n",
            "Epoch: 5 Training loss: 0.57 Validation loss: 0.56 Validation accuracy: 73.19%\n",
            "Epoch: 6 Training loss: 0.56 Validation loss: 0.56 Validation accuracy: 73.4%\n",
            "Epoch: 7 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.47%\n",
            "Epoch: 8 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.49%\n",
            "Epoch: 9 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.57%\n",
            "Epoch: 10 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.57%\n",
            "Epoch: 11 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.66%\n",
            "Epoch: 12 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.69%\n",
            "Epoch: 13 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.77%\n",
            "Epoch: 14 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.77%\n",
            "Epoch: 15 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.84%\n",
            "Epoch: 16 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.9%\n",
            "Epoch: 17 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.89%\n",
            "Epoch: 18 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.89%\n",
            "Epoch: 19 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.8%\n",
            "Epoch: 20 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.8%\n",
            "Epoch: 21 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.83%\n",
            "Epoch: 22 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.81%\n",
            "Epoch: 23 Training loss: 0.56 Validation loss: 0.55 Validation accuracy: 73.8%\n",
            "Epoch: 24 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.8%\n",
            "Epoch: 25 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.79%\n",
            "Epoch: 26 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.81%\n",
            "Epoch: 27 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.86%\n",
            "Epoch: 28 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.76%\n",
            "Epoch: 29 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.74%\n",
            "Epoch: 30 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.73%\n",
            "Epoch: 31 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.79%\n",
            "Epoch: 32 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.81%\n",
            "Epoch: 33 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.83%\n",
            "Epoch: 34 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.84%\n",
            "Epoch: 35 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.84%\n",
            "Epoch: 36 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.81%\n",
            "Epoch: 37 Training loss: 0.55 Validation loss: 0.55 Validation accuracy: 73.86%\n",
            "Epoch: 38 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.86%\n",
            "Epoch: 39 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.84%\n",
            "Epoch: 40 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.93%\n",
            "Epoch: 41 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 42 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.94%\n",
            "Epoch: 43 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.97%\n",
            "Epoch: 44 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 45 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.99%\n",
            "Epoch: 46 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 47 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.0%\n",
            "Epoch: 48 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 49 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.97%\n",
            "Epoch: 50 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 51 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.96%\n",
            "Epoch: 52 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.01%\n",
            "Epoch: 53 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 73.97%\n",
            "Epoch: 54 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.01%\n",
            "Epoch: 55 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.06%\n",
            "Epoch: 56 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.07%\n",
            "Epoch: 57 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.1%\n",
            "Epoch: 58 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.1%\n",
            "Epoch: 59 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.16%\n",
            "Epoch: 60 Training loss: 0.55 Validation loss: 0.54 Validation accuracy: 74.2%\n",
            "Epoch: 61 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.19%\n",
            "Epoch: 62 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.2%\n",
            "Epoch: 63 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.19%\n",
            "Epoch: 64 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.23%\n",
            "Epoch: 65 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.23%\n",
            "Epoch: 66 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.21%\n",
            "Epoch: 67 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.24%\n",
            "Epoch: 68 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.24%\n",
            "Epoch: 69 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.26%\n",
            "Epoch: 70 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.26%\n",
            "Epoch: 71 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.26%\n",
            "Epoch: 72 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.27%\n",
            "Epoch: 73 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.26%\n",
            "Epoch: 74 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.37%\n",
            "Epoch: 75 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.34%\n",
            "Epoch: 76 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.36%\n",
            "Epoch: 77 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.39%\n",
            "Epoch: 78 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.34%\n",
            "Epoch: 79 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.4%\n",
            "Epoch: 80 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.4%\n",
            "Epoch: 81 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.37%\n",
            "Epoch: 82 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.33%\n",
            "Epoch: 83 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.31%\n",
            "Epoch: 84 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.34%\n",
            "Epoch: 85 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.37%\n",
            "Epoch: 86 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.37%\n",
            "Epoch: 87 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.34%\n",
            "Epoch: 88 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.36%\n",
            "Epoch: 89 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.39%\n",
            "Epoch: 90 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.43%\n",
            "Epoch: 91 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.47%\n",
            "Epoch: 92 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.5%\n",
            "Epoch: 93 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.49%\n",
            "Epoch: 94 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.5%\n",
            "Epoch: 95 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.53%\n",
            "Epoch: 96 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.49%\n",
            "Epoch: 97 Training loss: 0.54 Validation loss: 0.54 Validation accuracy: 74.41%\n",
            "Epoch: 98 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.4%\n",
            "Epoch: 99 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.39%\n",
            "Epoch: 100 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.4%\n",
            "Epoch: 101 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 102 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.49%\n",
            "Epoch: 103 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.44%\n",
            "Epoch: 104 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 105 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 106 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 107 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.41%\n",
            "Epoch: 108 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 109 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.47%\n",
            "Epoch: 110 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.5%\n",
            "Epoch: 111 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.46%\n",
            "Epoch: 112 Training loss: 0.54 Validation loss: 0.53 Validation accuracy: 74.46%\n",
            "Epoch: 113 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.41%\n",
            "Epoch: 114 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.46%\n",
            "Epoch: 115 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.47%\n",
            "Epoch: 116 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.49%\n",
            "Epoch: 117 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.49%\n",
            "Epoch: 118 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.49%\n",
            "Epoch: 119 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.51%\n",
            "Epoch: 120 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 121 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 122 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.44%\n",
            "Epoch: 123 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 124 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 125 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.43%\n",
            "Epoch: 126 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.44%\n",
            "Epoch: 127 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.5%\n",
            "Epoch: 128 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.54%\n",
            "Epoch: 129 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.54%\n",
            "Epoch: 130 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.53%\n",
            "Epoch: 131 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.53%\n",
            "Epoch: 132 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.54%\n",
            "Epoch: 133 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.56%\n",
            "Epoch: 134 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 135 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.61%\n",
            "Epoch: 136 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 137 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 138 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 139 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 140 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.69%\n",
            "Epoch: 141 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 142 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 143 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.67%\n",
            "Epoch: 144 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.63%\n",
            "Epoch: 145 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 146 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.57%\n",
            "Epoch: 147 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.61%\n",
            "Epoch: 148 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.63%\n",
            "Epoch: 149 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.63%\n",
            "Epoch: 150 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 151 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.67%\n",
            "Epoch: 152 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 153 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 154 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.66%\n",
            "Epoch: 155 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.63%\n",
            "Epoch: 156 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 157 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.64%\n",
            "Epoch: 158 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.63%\n",
            "Epoch: 159 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.6%\n",
            "Epoch: 160 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.56%\n",
            "Epoch: 161 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.53%\n",
            "Epoch: 162 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.57%\n",
            "Epoch: 163 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 164 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 165 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.6%\n",
            "Epoch: 166 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.61%\n",
            "Epoch: 167 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 168 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 169 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.6%\n",
            "Epoch: 170 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.56%\n",
            "Epoch: 171 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.59%\n",
            "Epoch: 172 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.6%\n",
            "Epoch: 173 Training loss: 0.53 Validation loss: 0.53 Validation accuracy: 74.6%\n",
            "End of training. Training took 253.43 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcLm_IfqpDWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c32a0133-f60c-446c-a0f7-b059bf3c0d79"
      },
      "cell_type": "code",
      "source": [
        "# forward propagating the test dataset\n",
        "test_data = DatasetHandler(test_npz)\n",
        "\n",
        "for input_batch, target_batch in test_data:\n",
        "    test_accuracy = sess.run(\n",
        "        accuracy,\n",
        "        feed_dict={inputs: input_batch, targets: target_batch}\n",
        "    )\n",
        "\n",
        "test_accuracy_percent = round(test_accuracy * 100, 2)\n",
        "print(f\"Test accuracy: {test_accuracy_percent}%\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 73.64%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D_HEdqxwiD3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trying now other classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVjWqkvTnl2Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reg = LogisticRegression(solver=\"liblinear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-dTJIXkUntAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "npz = np.load(\"ml-datasets/train.npz\")\n",
        "x_train = npz[\"inputs\"].astype(np.float32)\n",
        "y_train = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_uCdz5RobG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "450ca463-f975-4c0a-892c-6624ca194d33"
      },
      "cell_type": "code",
      "source": [
        "reg.fit(x_train, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "afXHI6IXoeyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# validation dataset\n",
        "npz = np.load(\"ml-datasets/validation.npz\")\n",
        "x_valid = npz[\"inputs\"].astype(np.float32)\n",
        "y_valid = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWp7x55xom2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41524cc3-53fe-4253-d2d9-fb81b206f8b0"
      },
      "cell_type": "code",
      "source": [
        "reg.score(x_valid, y_valid)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7328571428571429"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "ixggUmIeorya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test dataset\n",
        "npz = np.load(\"ml-datasets/test.npz\")\n",
        "x_test = npz[\"inputs\"].astype(np.float32)\n",
        "y_test = npz[\"targets\"].astype(np.int8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gyVhHk9io6hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3b73b62-56e1-4357-e63e-ec6539ebd65d"
      },
      "cell_type": "code",
      "source": [
        "reg.score(x_test, y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7262857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "dK7xw_8io9EJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}